{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from labml_helpers.module import Module\n",
    "from labml_nn.transformers import TransformerLayer\n",
    "from labml_nn.utils import clone_module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(Module):\n",
    "    def __init__(self, d_model: int, patch_size: int, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, d_model, patch_size, stride=patch_size)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv(x)\n",
    "        bs, c, h, w = x.shape\n",
    "        x = x.permute(2, 3, 0, 1)\n",
    "        x = x.view(h* w, bs, c)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbeddings(Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5_000):\n",
    "        super().__init__()\n",
    "        self.positional_encodings = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        pe = self.positional_encodings[:x.shape[0]]\n",
    "        return x + pe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(Module):\n",
    "    def __init__(self, d_model: int, n_hidden: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, n_hidden)\n",
    "        self.act = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(n_hidden, n_classes)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.act(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(Module):\n",
    "    def __init__(self, transformer_layer: TransformerLayer, n_layers: int,\n",
    "        patch_emb: PatchEmbeddings, pos_emb: LearnedPositionalEmbeddings,\n",
    "        classification: ClassificationHead):\n",
    "        super().__init__()\n",
    "        self.patch_emb = patch_emb\n",
    "        self.pos_emb = pos_emb\n",
    "        self.classification = classification\n",
    "        self.transformer_layers = clone_module_list(transformer_layer, n_layers)\n",
    "        self.cls_token_emb = nn.Parameter(torch.randn(1, 1, transformer_layer.size), requires_grad=True)\n",
    "        self.ln = nn.LayerNorm([transformer_layer.size])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.patch_emb(x)\n",
    "        cls_token_emb = self.cls_token_emb.expand(-1, x.shape[1], -1)\n",
    "        x = torch.cat([cls_token_emb, x])\n",
    "        x = self.pos_emb(x)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x=x, mask=None)\n",
    "        \n",
    "        x = x[0]\n",
    "        x = self.ln(x)\n",
    "        x = self.classification(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
